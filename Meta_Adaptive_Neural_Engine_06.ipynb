{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOTGWJ73/e6VL+ZBYNEO2F4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CogNetSys/MetaStrata/blob/main/Meta_Adaptive_Neural_Engine_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8y39PfNlTgpX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvttuaTiS7jL"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# MANE (Meta-Adaptive Neural Engine)\n",
        "### A Self-Evolving, Model-Agnostic Platform for AGI\n",
        "MANE is designed to continuously evolve its architecture and learning strategy through adaptive clustering, evolutionary optimization, and maximum dissimilarity learning. It leverages self-organizing clustering mechanisms, hyperbolic-inspired distance computations, and a reinforcement learning-based hyperparameter feedback loop. This system is built to be model-agnostic, enabling it to modify its own structure dynamically and to eventually decide the optimal learning framework for its tasks.\n",
        "\"\"\"\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "## Cell 1: Install Dependencies\n",
        "Install the necessary packages. We'll need standard deep learning libraries, as well as libraries for reinforcement learning, clustering, and (optionally) quantum computing simulations.\n",
        "\"\"\"\n",
        "\n",
        "# %%\n",
        "!pip install torch torchvision scikit-learn tensorboard faiss-cpu scikit-optimize\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "## Cell 2: Imports & Logging Configuration\n",
        "Import libraries, set up logging, configure TensorBoard, and import modules for quantum simulation.\n",
        "This final version includes comprehensive console logging and progress indicators.\n",
        "\"\"\"\n",
        "# %%\n",
        "#######################\n",
        "# Imports & Logging   #\n",
        "#######################\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "import json\n",
        "import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split  # Make sure this is imported!\n",
        "\n",
        "# For Bayesian optimization and progress indication\n",
        "from sklearn.base import BaseEstimator\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real\n",
        "from tqdm import tqdm\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "writer = SummaryWriter('runs/Combined_Experiment')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# Cell 3: Data Preprocessing & Augmentation (Phase 2.5 - Dataset Diversification)\n",
        "This cell loads one of several datasets. You can choose from:\n",
        "  - Standard datasets: \"Iris\", \"Wine\", \"Titanic\", \"Digits\", \"Fashion-MNIST\"\n",
        "  - Multimodal datasets: \"MELD\", \"IEMOCAP\", \"LUMA\", \"M3AV\"\n",
        "For multimodal datasets, we assume a CSV file is available with (at least) a 'text' and 'label' column.\n",
        "For standard datasets, we use scikit‑learn or torchvision.\n",
        "\"\"\"\n",
        "# %%\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set your desired dataset name here:\n",
        "# Options for standard datasets: \"Iris\", \"Wine\", \"Titanic\", \"Digits\", \"Fashion-MNIST\"\n",
        "# Options for multimodal datasets: \"MELD\", \"IEMOCAP\", \"LUMA\", \"M3AV\"\n",
        "dataset_name = \"Iris\"  # <-- change this value to test different datasets\n",
        "\n",
        "multimodal_datasets = {\"MELD\", \"IEMOCAP\", \"LUMA\", \"M3AV\"}\n",
        "enable_multimodal = dataset_name in multimodal_datasets\n",
        "\n",
        "if dataset_name == \"MELD\":\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        df = pd.read_csv('meld_data.csv')\n",
        "        vectorizer = TfidfVectorizer(max_features=10)\n",
        "        text_features = vectorizer.fit_transform(df['text']).toarray()\n",
        "        np.random.seed(42)\n",
        "        audio_features = np.random.normal(0, 1, size=(df.shape[0], 3))\n",
        "        video_features = np.random.normal(0, 1, size=(df.shape[0], 5))\n",
        "        X = np.concatenate([text_features, audio_features, video_features], axis=1)\n",
        "        y = df['label'].values\n",
        "        print(\"Loaded MELD dataset.\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not load MELD dataset; falling back to Iris.\")\n",
        "        dataset_name = \"Iris\"\n",
        "        enable_multimodal = False\n",
        "\n",
        "elif dataset_name == \"IEMOCAP\":\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        df = pd.read_csv('iemocap_data.csv')\n",
        "        vectorizer = TfidfVectorizer(max_features=10)\n",
        "        text_features = vectorizer.fit_transform(df['text']).toarray()\n",
        "        np.random.seed(42)\n",
        "        audio_features = np.random.normal(0, 1, size=(df.shape[0], 3))\n",
        "        video_features = np.random.normal(0, 1, size=(df.shape[0], 5))\n",
        "        X = np.concatenate([text_features, audio_features, video_features], axis=1)\n",
        "        y = df['label'].values\n",
        "        print(\"Loaded IEMOCAP dataset.\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not load IEMOCAP dataset; falling back to Iris.\")\n",
        "        dataset_name = \"Iris\"\n",
        "        enable_multimodal = False\n",
        "\n",
        "elif dataset_name == \"LUMA\":\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        df = pd.read_csv('luma_data.csv')\n",
        "        vectorizer = TfidfVectorizer(max_features=10)\n",
        "        text_features = vectorizer.fit_transform(df['text']).toarray()\n",
        "        np.random.seed(42)\n",
        "        image_features = np.random.normal(0, 1, size=(df.shape[0], 4))\n",
        "        audio_features = np.random.normal(0, 1, size=(df.shape[0], 3))\n",
        "        X = np.concatenate([text_features, image_features, audio_features], axis=1)\n",
        "        y = df['label'].values\n",
        "        print(\"Loaded LUMA dataset.\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not load LUMA dataset; falling back to Iris.\")\n",
        "        dataset_name = \"Iris\"\n",
        "        enable_multimodal = False\n",
        "\n",
        "elif dataset_name == \"M3AV\":\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        df = pd.read_csv('m3av_data.csv')\n",
        "        vectorizer = TfidfVectorizer(max_features=10)\n",
        "        text_features = vectorizer.fit_transform(df['text']).toarray()\n",
        "        np.random.seed(42)\n",
        "        speech_features = np.random.normal(0, 1, size=(df.shape[0], 3))\n",
        "        visual_features = np.random.normal(0, 1, size=(df.shape[0], 5))\n",
        "        X = np.concatenate([text_features, speech_features, visual_features], axis=1)\n",
        "        y = df['label'].values\n",
        "        print(\"Loaded M3AV dataset.\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not load M3AV dataset; falling back to Iris.\")\n",
        "        dataset_name = \"Iris\"\n",
        "        enable_multimodal = False\n",
        "\n",
        "elif dataset_name == \"Iris\":\n",
        "    from sklearn import datasets\n",
        "    iris = datasets.load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    np.random.seed(42)\n",
        "    extra_features = np.random.normal(0, 1, size=(X.shape[0], 3))\n",
        "    X = np.concatenate([X, extra_features], axis=1)\n",
        "    print(\"Using Iris dataset with simulated extra features.\")\n",
        "\n",
        "elif dataset_name == \"Wine\":\n",
        "    from sklearn import datasets\n",
        "    wine = datasets.load_wine()\n",
        "    X = wine.data\n",
        "    y = wine.target\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    np.random.seed(42)\n",
        "    extra_features = np.random.normal(0, 1, size=(X.shape[0], 2))\n",
        "    X = np.concatenate([X, extra_features], axis=1)\n",
        "    print(\"Using Wine dataset with simulated extra features.\")\n",
        "\n",
        "elif dataset_name == \"Titanic\":\n",
        "    try:\n",
        "        import seaborn as sns\n",
        "        df = sns.load_dataset(\"titanic\").dropna()\n",
        "        X = df[['pclass', 'age', 'sibsp', 'parch', 'fare']].values\n",
        "        y = df[\"survived\"].values\n",
        "        scaler = StandardScaler()\n",
        "        X = scaler.fit_transform(X)\n",
        "        np.random.seed(42)\n",
        "        extra_features = np.random.normal(0, 1, size=(X.shape[0], 2))\n",
        "        X = np.concatenate([X, extra_features], axis=1)\n",
        "        print(\"Using Titanic dataset with simulated extra features.\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not load Titanic dataset; falling back to Iris.\")\n",
        "        dataset_name = \"Iris\"\n",
        "        enable_multimodal = False\n",
        "\n",
        "elif dataset_name == \"Digits\":\n",
        "    from sklearn import datasets\n",
        "    digits = datasets.load_digits()\n",
        "    X = digits.data\n",
        "    y = digits.target\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    np.random.seed(42)\n",
        "    extra_features = np.random.normal(0, 1, size=(X.shape[0], 2))\n",
        "    X = np.concatenate([X, extra_features], axis=1)\n",
        "    print(\"Using Digits dataset with simulated extra features.\")\n",
        "\n",
        "elif dataset_name == \"Fashion-MNIST\":\n",
        "    from torchvision import datasets, transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.view(-1))\n",
        "    ])\n",
        "    fashion = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    X = np.array([np.array(img) for img, _ in fashion])\n",
        "    y = np.array([label for _, label in fashion])\n",
        "    X = X / 255.0\n",
        "    X = X.reshape(X.shape[0], -1)\n",
        "    np.random.seed(42)\n",
        "    extra_features = np.random.normal(0, 1, size=(X.shape[0], 2))\n",
        "    X = np.concatenate([X, extra_features], axis=1)\n",
        "    print(\"Using Fashion-MNIST dataset with simulated extra features.\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Dataset '{dataset_name}' not recognized. Please choose from Iris, Wine, Titanic, Digits, Fashion-MNIST, MELD, IEMOCAP, LUMA, or M3AV.\")\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y_tensor)\n",
        "X_train = X_train.to(device)\n",
        "X_val = X_val.to(device)\n",
        "y_train = y_train.to(device)\n",
        "y_val = y_val.to(device)\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "print(f\"Dataset: {dataset_name} | Multi‐Modal Enabled: {enable_multimodal}\")\n",
        "print(f\"Training data shape: {X_train.shape}, Validation data shape: {X_val.shape}\")\n",
        "print(f\"Unique classes: {torch.unique(y_tensor)}\")\n",
        "\n",
        "# Compute class weights for bias mitigation\n",
        "class_counts = torch.bincount(y_train)\n",
        "class_weights = 1.0 / (class_counts.float() + 1e-8)\n",
        "class_weights = class_weights / class_weights.sum()\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "def mixup(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    index = torch.randperm(x.size(0)).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    num_classes = int(torch.max(y)) + 1\n",
        "    y_onehot = torch.nn.functional.one_hot(y, num_classes=num_classes).float()\n",
        "    mixed_y = lam * y_onehot + (1 - lam) * y_onehot[index, :]\n",
        "    return mixed_x, mixed_y\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# Cell 4: Utility Functions (Phase 2.5 - Next Iteration)\n",
        "Defines functions for various metrics, uncertainty estimation, fairness, robust loss, zero-shot evaluation,\n",
        "sensor noise simulation, prediction distribution, and a dummy bias correction.\n",
        "\"\"\"\n",
        "# %%\n",
        "def hyperbolic_distance(x, y):\n",
        "    x = torch.tensor(x, dtype=torch.float32, device=device)\n",
        "    y = torch.tensor(y, dtype=torch.float32, device=device)\n",
        "    norm_x = torch.norm(x)\n",
        "    norm_y = torch.norm(y)\n",
        "    return torch.acosh(1 + 2 * torch.sum((x - y)**2) / ((1 - norm_x**2) * (1 - norm_y**2)))\n",
        "\n",
        "def compute_centroid_entropy(centroids):\n",
        "    with torch.no_grad():\n",
        "        D = torch.cdist(centroids, centroids, p=2)\n",
        "        D.fill_diagonal_(float('inf'))\n",
        "        p = 1 / (D + 1e-8)\n",
        "        p = p / p.sum(dim=1, keepdim=True)\n",
        "        p = torch.clamp(p, min=1e-8)\n",
        "        entropy_per_centroid = -torch.sum(p * torch.log(p), dim=1)\n",
        "        return torch.mean(entropy_per_centroid)\n",
        "\n",
        "def enforce_centroid_separation(centroids, min_sep=1.0):\n",
        "    with torch.no_grad():\n",
        "        num_centroids = centroids.size(0)\n",
        "        for i in range(num_centroids):\n",
        "            for j in range(i+1, num_centroids):\n",
        "                diff = centroids[i] - centroids[j]\n",
        "                dist = torch.norm(diff)\n",
        "                if dist < min_sep:\n",
        "                    correction = (min_sep - dist) / 2\n",
        "                    direction = diff / (dist + 1e-8)\n",
        "                    centroids[i].add_(correction * direction)\n",
        "                    centroids[j].add_(-correction * direction)\n",
        "\n",
        "def monte_carlo_dropout(model, x, epoch, num_samples=10):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    for _ in range(num_samples):\n",
        "        with torch.amp.autocast('cuda', enabled=True):\n",
        "            outputs = model(x, epoch)\n",
        "        preds.append(torch.softmax(outputs, dim=1).unsqueeze(0))\n",
        "    preds = torch.cat(preds, dim=0)\n",
        "    mean_preds = preds.mean(dim=0)\n",
        "    variance = preds.var(dim=0).mean().item()\n",
        "    return mean_preds, variance\n",
        "\n",
        "def monte_carlo_dropout_per_sample(model, x, epoch, num_samples=10):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    for _ in range(num_samples):\n",
        "        with torch.amp.autocast('cuda', enabled=True):\n",
        "            outputs = model(x, epoch)\n",
        "        preds.append(torch.softmax(outputs, dim=1).unsqueeze(0))\n",
        "    preds = torch.cat(preds, dim=0)\n",
        "    sample_variances = preds.var(dim=0).mean(dim=1)\n",
        "    return sample_variances\n",
        "\n",
        "def fairness_metric(pred_probs, y_true):\n",
        "    preds = torch.argmax(pred_probs, dim=1)\n",
        "    unique, counts = torch.unique(y_true, return_counts=True)\n",
        "    ideal = torch.ones_like(counts, dtype=torch.float32) / len(unique)\n",
        "    actual = counts.float() / counts.sum()\n",
        "    bias = torch.norm(actual - ideal)\n",
        "    return bias.item()\n",
        "\n",
        "def log_energy_usage():\n",
        "    usage = torch.cuda.memory_allocated(device) / 1e6\n",
        "    return usage\n",
        "\n",
        "def compute_modal_consistency(x):\n",
        "    return 1.0  # For fallback datasets, we return 1.0.\n",
        "\n",
        "def meta_cognition_metric(pred_probs):\n",
        "    entropy = -torch.sum(pred_probs * torch.log(pred_probs + 1e-8), dim=1)\n",
        "    return entropy.mean().item()\n",
        "\n",
        "def compute_saliency_map(model, x, target_class=None):\n",
        "    model.eval()\n",
        "    x.requires_grad_()\n",
        "    outputs = model(x, 0)\n",
        "    if target_class is None:\n",
        "        target_class = outputs.argmax(dim=1)\n",
        "    loss = nn.CrossEntropyLoss()(outputs, target_class)\n",
        "    loss.backward()\n",
        "    saliency = x.grad.abs().mean().item()\n",
        "    x.requires_grad_(False)\n",
        "    return saliency\n",
        "\n",
        "weighted_loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "def robust_loss(x, outputs, targets, model, lambda_reg=0.01):\n",
        "    ce_loss = weighted_loss_fn(outputs, targets)\n",
        "    x_adv = x.clone().detach().requires_grad_(True)\n",
        "    outputs_adv = model(x_adv, 0)\n",
        "    loss_for_grad = weighted_loss_fn(outputs_adv, targets)\n",
        "    loss_for_grad.backward(retain_graph=True)\n",
        "    grad_norm = x_adv.grad.norm(2)\n",
        "    penalty = lambda_reg * grad_norm\n",
        "    return ce_loss + penalty\n",
        "\n",
        "def zero_shot_evaluation(model, num_samples=20):\n",
        "    synthetic_data = generate_zero_shot_data(num_samples, input_size)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(synthetic_data, 0)\n",
        "    predictions = torch.argmax(outputs, dim=1)\n",
        "    unique, counts = torch.unique(predictions, return_counts=True)\n",
        "    return dict(zip(unique.cpu().numpy(), counts.cpu().numpy()))\n",
        "\n",
        "def detect_training_failure(loss_history, patience=5, min_improvement=0.01):\n",
        "    if len(loss_history) < patience:\n",
        "        return False\n",
        "    recent = loss_history[-patience:]\n",
        "    if max(recent) - min(recent) < min_improvement:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def update_experience_replay(buffer, new_batch, max_size=100):\n",
        "    buffer.extend(new_batch)\n",
        "    if len(buffer) > max_size:\n",
        "        buffer = buffer[-max_size:]\n",
        "    return buffer\n",
        "\n",
        "def apply_adaptive_pruning(model, threshold=0.05):\n",
        "    with torch.no_grad():\n",
        "        for module in model.main_head:\n",
        "            if isinstance(module, nn.Linear):\n",
        "                module.weight.data = module.weight.data.masked_fill(module.weight.data.abs() < threshold, 0.0)\n",
        "\n",
        "def simulate_sensor_noise(x, noise_std=0.1):\n",
        "    noise = noise_std * torch.randn_like(x)\n",
        "    return x + noise\n",
        "\n",
        "def apply_bias_correction(pred_probs, bias_threshold=0.1):\n",
        "    return pred_probs\n",
        "\n",
        "def explain_with_shap(model, x):\n",
        "    import numpy as np\n",
        "    return np.random.uniform(0.0, 1.0)\n",
        "\n",
        "def compute_prediction_distribution(model, x, epoch=0):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x, epoch)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "    import numpy as np\n",
        "    unique, counts = np.unique(preds, return_counts=True)\n",
        "    return dict(zip(unique, counts))\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# Cell 5: RL Hyperparameter Agent (Phase 2.5 - Next Iteration)\n",
        "Updates hyperparameters based on various metrics.\n",
        "\"\"\"\n",
        "# %%\n",
        "class RLHyperparameterAgent:\n",
        "    def __init__(self, init_alpha=0.1, init_lr=0.01, energy_threshold=25.0):\n",
        "        self.alpha = init_alpha\n",
        "        self.lr = init_lr\n",
        "        self.max_centroid_sep = 20\n",
        "        self.energy_threshold = energy_threshold\n",
        "    def update(self, centroid_separation, fairness=None, modal_consistency=None, meta_cognition=None, energy_usage=None):\n",
        "        if fairness is not None and fairness > 0.1:\n",
        "            self.alpha = min(0.5, self.alpha * 1.06)\n",
        "            self.lr = min(0.05, self.lr * 1.06)\n",
        "        elif modal_consistency is not None and modal_consistency < 0.5:\n",
        "            self.alpha = min(0.5, self.alpha * 1.04)\n",
        "            self.lr = min(0.05, self.lr * 1.04)\n",
        "        elif meta_cognition is not None and meta_cognition < 0.8:\n",
        "            self.alpha = min(0.5, self.alpha * 1.05)\n",
        "            self.lr = min(0.05, self.lr * 1.05)\n",
        "        elif energy_usage is not None and energy_usage > self.energy_threshold:\n",
        "            self.lr = max(1e-4, self.lr * 0.95)\n",
        "        elif centroid_separation < 1.0:\n",
        "            self.alpha = min(0.5, self.alpha * 1.03)\n",
        "            self.lr = min(0.05, self.lr * 1.01)\n",
        "        else:\n",
        "            self.alpha = max(0.01, self.alpha * 0.99)\n",
        "            self.lr = max(1e-4, self.lr * 0.99)\n",
        "        centroid_separation = min(self.max_centroid_sep, centroid_separation)\n",
        "        return self.alpha, self.lr, centroid_separation\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# Cell 6: Evolvable Model Architectures (NAS Module) (Phase 2.5 - Next Iteration)\n",
        "Builds an evolvable MLP candidate with support for few-shot mode, adaptive pruning, and meta-cognition.\n",
        "Also adds a method to mutate the architecture.\n",
        "\"\"\"\n",
        "# %%\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "def build_evolvable_model(candidate, input_size, num_classes, total_epochs=20):\n",
        "    arch = candidate[\"architecture\"]\n",
        "    few_shot = candidate.get(\"few_shot\", False)\n",
        "    adaptive_pruning = candidate.get(\"adaptive_pruning\", False)\n",
        "    meta_cognition_enabled = candidate.get(\"meta_cognition\", False)\n",
        "    if arch == \"MLP\":\n",
        "        class EvolvableMLP(nn.Module):\n",
        "            def __init__(self, candidate, input_size, num_classes, total_epochs):\n",
        "                super(EvolvableMLP, self).__init__()\n",
        "                self.candidate = candidate  # store candidate for cloning\n",
        "                self.adaptive_pruning = adaptive_pruning\n",
        "                self.meta_cognition_enabled = meta_cognition_enabled\n",
        "                layers = []\n",
        "                in_features = input_size\n",
        "                if few_shot:\n",
        "                    meta_hidden = max(16, candidate[\"hidden_size\"] // 2)\n",
        "                    layers.append(nn.Linear(in_features, meta_hidden))\n",
        "                    layers.append(nn.ReLU())\n",
        "                    in_features = meta_hidden\n",
        "                num_layers = candidate[\"num_layers\"]\n",
        "                hidden_size = candidate[\"hidden_size\"]\n",
        "                dropout_rate = candidate[\"dropout\"]\n",
        "                activations = [nn.ReLU(), nn.SiLU(), nn.LeakyReLU()]\n",
        "                for i in range(num_layers):\n",
        "                    layers.append(nn.Linear(in_features, hidden_size))\n",
        "                    layers.append(random.choice(activations))\n",
        "                    layers.append(nn.Dropout(dropout_rate))\n",
        "                    in_features = hidden_size\n",
        "                self.main_head = nn.Sequential(*layers)\n",
        "                self.classifier = nn.Linear(in_features, num_classes)\n",
        "                if self.meta_cognition_enabled:\n",
        "                    self.reflection_head = nn.Linear(in_features, 1)\n",
        "                self.total_epochs = total_epochs\n",
        "\n",
        "            def forward(self, x, epoch=None):\n",
        "                features = self.main_head(x)\n",
        "                out = self.classifier(features)\n",
        "                if self.meta_cognition_enabled:\n",
        "                    self.reflection_output = self.reflection_head(features)\n",
        "                self.latent_features = features\n",
        "                return out\n",
        "\n",
        "            def mutate_architecture(self):\n",
        "                for module in self.main_head:\n",
        "                    if isinstance(module, nn.Dropout):\n",
        "                        new_rate = module.p + random.uniform(-0.05, 0.05)\n",
        "                        module.p = max(0.0, min(new_rate, 0.5))\n",
        "                        break\n",
        "\n",
        "        return EvolvableMLP(candidate, input_size, num_classes, total_epochs)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown architecture in candidate: {arch}\")\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# Cell 7: Custom Estimator for Hyperparameter Optimization & Automated Retraining (Phase 2.5 - Next Iteration)\n",
        "This estimator integrates multiple self-improvement features.\n",
        "Note: Instead of using copy.deepcopy, we now use a custom cloning function to re-create a fresh model from the stored candidate.\n",
        "\"\"\"\n",
        "# %%\n",
        "from sklearn.base import BaseEstimator\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def adaptive_alpha(epoch, base_alpha=0.01, increase_factor=1.05, max_alpha=0.1):\n",
        "    return min(base_alpha * (increase_factor ** epoch), max_alpha)\n",
        "\n",
        "def neural_network_competition(X, y, population, num_epochs_comp=3, batch_size=16):\n",
        "    best_model = None\n",
        "    best_loss = float('inf')\n",
        "    for model in population:\n",
        "        model.train()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "        total_loss = 0.0\n",
        "        for comp_epoch in range(num_epochs_comp):\n",
        "            permutation = torch.randperm(X.size(0))\n",
        "            for i in range(0, X.size(0), batch_size):\n",
        "                indices = permutation[i:i+batch_size]\n",
        "                batch = X[indices]\n",
        "                batch_labels = y[indices]\n",
        "                optimizer.zero_grad()\n",
        "                with torch.amp.autocast('cuda', enabled=True):\n",
        "                    outputs = model(batch, comp_epoch)\n",
        "                    loss = nn.CrossEntropyLoss(weight=class_weights)(outputs, batch_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "        if total_loss < best_loss:\n",
        "            best_loss = total_loss\n",
        "            best_model = model.state_dict()  # store state dict of best model\n",
        "    return best_model\n",
        "\n",
        "class MANEEstimator(BaseEstimator):\n",
        "    def __init__(self, input_size, hidden_size, latent_size, num_classes,\n",
        "                 entropy_weight=0.1, min_sep=1.0, base_alpha=0.01, total_epochs=35,\n",
        "                 confidence_threshold=0.05, use_curriculum=True, self_directed=False):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.latent_size = latent_size\n",
        "        self.num_classes = num_classes\n",
        "        self.entropy_weight = entropy_weight\n",
        "        self.min_sep = min_sep\n",
        "        self.base_alpha = base_alpha\n",
        "        self.total_epochs = total_epochs\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.use_curriculum = use_curriculum\n",
        "        self.self_directed = self_directed\n",
        "        # Build model with candidate dictionary\n",
        "        self.candidate = {\n",
        "            \"architecture\": \"MLP\",\n",
        "            \"num_layers\": 3,\n",
        "            \"hidden_size\": hidden_size,\n",
        "            \"dropout\": 0.35,\n",
        "            \"few_shot\": False,\n",
        "            \"adaptive_pruning\": False,\n",
        "            \"meta_cognition\": True\n",
        "        }\n",
        "        self.model = build_evolvable_model(self.candidate, input_size, num_classes, total_epochs=self.total_epochs)\n",
        "        self.model.to(device)\n",
        "        self.optimizer = None\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        self.best_model = None\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.no_improve_epochs = 0\n",
        "        self.experience_replay_buffer = []\n",
        "        self.loss_history = []\n",
        "\n",
        "    def fit(self, X, y, learning_rate=0.005, num_epochs=35, batch_size=64, weight_decay=5e-3,\n",
        "            patience=5, auto_grok_min_epochs=10, auto_grok_threshold=0.04):\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "        self.no_improve_epochs = 0\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=2, T_mult=1, eta_min=5e-6)\n",
        "\n",
        "        # Cloning function to safely duplicate model without deepcopy()\n",
        "        def clone_fn(model):\n",
        "            new_model = build_evolvable_model(model.candidate, self.input_size, self.num_classes, self.total_epochs)\n",
        "            new_model.load_state_dict(model.state_dict())\n",
        "            new_model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "            return new_model\n",
        "\n",
        "        # Instead of deepcopy, safely clone model using clone_fn()\n",
        "        population = [clone_fn(self.model) for _ in range(3)]\n",
        "\n",
        "        # Reset parameters if necessary\n",
        "        for clone in population:\n",
        "            try:\n",
        "                clone.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Run neural network competition\n",
        "        best_population_state = neural_network_competition(X, y, population, num_epochs_comp=3, batch_size=batch_size)\n",
        "        if best_population_state is not None:\n",
        "            self.model.load_state_dict(best_population_state)\n",
        "\n",
        "        self.loss_history = []\n",
        "        exp_buffer = []  # Local experience replay buffer\n",
        "        for epoch in tqdm(range(num_epochs), desc=\"Retraining Epochs\"):\n",
        "            current_alpha = adaptive_alpha(epoch, base_alpha=0.01)\n",
        "            if hasattr(self.model, 'base_alpha'):\n",
        "                self.model.base_alpha = current_alpha\n",
        "\n",
        "            if self.use_curriculum:\n",
        "                curriculum_X, curriculum_y = generate_curriculum(X, y, self.model, num_samples=int(0.1 * X.size(0)))\n",
        "                X_epoch = torch.cat([X, curriculum_X])\n",
        "                y_epoch = torch.cat([y, curriculum_y])\n",
        "            else:\n",
        "                X_epoch, y_epoch = X, y\n",
        "\n",
        "            if epoch % 3 == 0:\n",
        "                self.model.mutate_architecture()\n",
        "                adv_examples = generate_adversarial_examples(X_epoch, y_epoch, self.model, epsilon=0.05)\n",
        "                X_epoch = torch.cat([X_epoch, adv_examples])\n",
        "                y_epoch = torch.cat([y_epoch, y_epoch])\n",
        "\n",
        "            self.model.train()\n",
        "            permutation = torch.randperm(X_epoch.size(0))\n",
        "            epoch_loss = 0.0\n",
        "            for i in range(0, X_epoch.size(0), batch_size):\n",
        "                indices = permutation[i:i+batch_size]\n",
        "                batch = X_epoch[indices]\n",
        "                batch_labels = y_epoch[indices]\n",
        "                self.optimizer.zero_grad()\n",
        "                with torch.amp.autocast('cuda', enabled=True):\n",
        "                    outputs = self.model(batch, epoch)\n",
        "                    loss = robust_loss(batch, outputs, batch_labels, self.model, lambda_reg=0.01)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                self.optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "                exp_buffer = update_experience_replay(exp_buffer, [(batch, batch_labels)])\n",
        "\n",
        "            scheduler.step()\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(X, epoch)\n",
        "                val_loss = self.criterion(outputs, y).mean()\n",
        "            self.loss_history.append(val_loss.item())\n",
        "\n",
        "            print(f\"Epoch {epoch+1}: Train Loss={epoch_loss:.4f}, Val Loss={val_loss.item():.4f}\")\n",
        "\n",
        "            if self.best_val_loss - val_loss.item() > 0.01:\n",
        "                self.no_improve_epochs = 0\n",
        "                self.best_val_loss = val_loss.item()\n",
        "                self.best_model = self.model.state_dict()\n",
        "            else:\n",
        "                self.no_improve_epochs += 1\n",
        "                if self.no_improve_epochs >= patience:\n",
        "                    print(\"Early stopping triggered.\")\n",
        "                    break\n",
        "\n",
        "        if self.best_model is not None:\n",
        "            self.model.load_state_dict(self.best_model)\n",
        "\n",
        "        stats_report = {\"retraining_runs\": []}\n",
        "        stats_report[\"retraining_runs\"].append({\n",
        "            \"run\": epoch+1,\n",
        "            \"best_val_loss\": self.best_val_loss,\n",
        "            \"total_epochs\": num_epochs\n",
        "        })\n",
        "\n",
        "        print(f\"Retraining run complete.\")\n",
        "        return self.model, stats_report\n",
        "\n",
        "# Instantiate and run training\n",
        "mane_model, retraining_stats = automated_retraining(X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Simple wrapper for AutoML evaluation\n",
        "from sklearn.base import BaseEstimator\n",
        "class DummyEstimator(BaseEstimator):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "    def score(self, X, y):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X, 0)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            return (preds == y).float().mean().item()\n",
        "\n",
        "estimator_wrapper = DummyEstimator(mane_model)\n",
        "automl_accuracy = estimator_wrapper.score(X_val, y_val)\n",
        "print(f\"AutoML Selected Model Accuracy: {automl_accuracy*100:.2f}%\")\n",
        "\n",
        "mane_writer.add_scalar(\"Final_Val_Accuracy\", automl_accuracy)\n",
        "mane_writer.close()\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "X_baseline = X_train.cpu().numpy()\n",
        "y_baseline = y_train.cpu().numpy()\n",
        "X_val_baseline = X_val.cpu().numpy()\n",
        "y_val_baseline = y_val.cpu().numpy()\n",
        "baseline_model = LogisticRegression(max_iter=400)\n",
        "baseline_model.fit(X_baseline, y_baseline)\n",
        "baseline_preds = baseline_model.predict(X_val_baseline)\n",
        "baseline_probs = baseline_model.predict_proba(X_val_baseline)\n",
        "baseline_acc = accuracy_score(y_val_baseline, baseline_preds)\n",
        "baseline_logloss = log_loss(y_val_baseline, baseline_probs)\n",
        "print(f\"Baseline Accuracy: {baseline_acc*100:.2f}%\")\n",
        "print(f\"Baseline Log Loss: {baseline_logloss:.4f}\")\n",
        "baseline_writer.add_scalar(\"Final_Val_Accuracy\", baseline_acc)\n",
        "baseline_writer.add_scalar(\"Final_Log_Loss\", baseline_logloss)\n",
        "baseline_writer.close()\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# Cell 10: Evaluate and Visualize the Learned Latent Space (Phase 2.5 - Next Iteration)\n",
        "Visualizes the latent space using PCA and annotates energy usage, modal consistency, meta‑cognition,\n",
        "and saliency. It also simulates edge deployment by reporting the model’s parameter count and inference time,\n",
        "and plots a histogram of the prediction distribution (for bias analysis).\n",
        "\"\"\"\n",
        "# %%\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import time\n",
        "\n",
        "def simulate_edge_deployment(model):\n",
        "    model_cpu = model.to(\"cpu\")\n",
        "    param_count = sum(p.numel() for p in model_cpu.parameters())\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        dummy_input = torch.randn(1, input_size)\n",
        "        _ = model_cpu(dummy_input, 0)\n",
        "    inference_time = time.time() - start\n",
        "    return param_count, inference_time\n",
        "\n",
        "def reset_vram():\n",
        "    print(\"🔄 Resetting VRAM...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    print(\"✅ VRAM has been cleared.\")\n",
        "\n",
        "if hasattr(mane_model, 'latent_features') and mane_model.latent_features is not None:\n",
        "    mane_model.eval()\n",
        "    with torch.no_grad():\n",
        "        _ = mane_model(X_train, 0)\n",
        "        latent_reps = mane_model.latent_features.cpu().numpy()\n",
        "    pca = PCA(n_components=2)\n",
        "    latent_2d = pca.fit_transform(latent_reps)\n",
        "    y_train_np = y_train.cpu().numpy()\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.scatter(latent_2d[:,0], latent_2d[:,1], c=y_train_np, cmap='viridis', alpha=0.7)\n",
        "    if hasattr(mane_model, 'centroids'):\n",
        "        centroids_np = mane_model.centroids.detach().cpu().numpy()\n",
        "        centroids_2d = pca.transform(centroids_np)\n",
        "        plt.scatter(centroids_2d[:,0], centroids_2d[:,1], marker='x', s=200, c='red', label='Centroids')\n",
        "    energy_usage = log_energy_usage()\n",
        "    modal_consistency = compute_modal_consistency(X_train)\n",
        "    meta_score = meta_cognition_metric(torch.softmax(mane_model(X_train, 0), dim=1))\n",
        "    sample_saliency = compute_saliency_map(mane_model, X_val[:10], target_class=y_val[:10])\n",
        "    param_count, inference_time = simulate_edge_deployment(mane_model)\n",
        "    pred_dist = compute_prediction_distribution(mane_model, X_train, epoch=0)\n",
        "    plt.figtext(0.15, 0.85, f\"Energy: {energy_usage:.2f} MB\\nModal Consistency: {modal_consistency:.2f}\\nMeta-Cognition: {meta_score:.2f}\\nSaliency: {sample_saliency:.2f}\\nParams: {param_count}\\nInference: {inference_time*1000:.2f} ms\\nPred Dist: {pred_dist}\", fontsize=10)\n",
        "    plt.title(f\"Latent Space Visualization for {dataset_name}\")\n",
        "    plt.xlabel(\"PCA Component 1\")\n",
        "    plt.ylabel(\"PCA Component 2\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"The selected architecture does not provide latent features for visualization.\")\n"
      ],
      "metadata": {
        "id": "pJYCAvfPULHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a-EGqUdCIThe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs/iris/"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EBTrYGe6IR9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs/Wine/"
      ],
      "metadata": {
        "id": "eqibNCoWITB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs/breast_cancer/"
      ],
      "metadata": {
        "id": "Gq-XGPF1ITKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs/mnist/"
      ],
      "metadata": {
        "id": "4iKlAyTpITRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs/fashion_mnist/"
      ],
      "metadata": {
        "id": "rGDQxpn7ITY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# Cell 11: VRAM Reset Utility\n",
        "This cell defines a function to reset VRAM by clearing CUDA cache and running garbage collection.\n",
        "Call reset_vram() after each candidate evaluation and retraining run.\n",
        "\"\"\"\n",
        "\n",
        "# %%\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def reset_vram():\n",
        "    \"\"\"Clears VRAM by emptying the CUDA cache and running garbage collection.\"\"\"\n",
        "    print(\"🔄 Resetting VRAM...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"✅ VRAM has been cleared.\")\n",
        "\n",
        "# Example call after a training iteration (already called within candidate loop and retraining).\n",
        "reset_vram()\n",
        "\n"
      ],
      "metadata": {
        "id": "LPIiAcDZrkMq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}