{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOTGWJ73/e6VL+ZBYNEO2F4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CogNetSys/MetaStrata/blob/main/Meta_Adaptive_Neural_Engine_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8y39PfNlTgpX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvttuaTiS7jL",
        "outputId": "4d22392e-6439-437a-99e3-f5ab69df9f08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (4.25.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, scikit-optimize, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed faiss-cpu-1.10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyaml-25.1.0 scikit-optimize-0.10.2\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# MANE (Meta-Adaptive Neural Engine)\n",
        "### A Self-Evolving, Model-Agnostic Platform for AGI\n",
        "MANE is designed to continuously evolve its architecture and learning strategy through adaptive clustering, evolutionary optimization, and maximum dissimilarity learning. It leverages self-organizing clustering mechanisms, hyperbolic-inspired distance computations, and a reinforcement learning-based hyperparameter feedback loop. This system is built to be model-agnostic, enabling it to modify its own structure dynamically and to eventually decide the optimal learning framework for its tasks.\n",
        "\"\"\"\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "## Cell 1: Install Dependencies\n",
        "Install the necessary packages. We'll need standard deep learning libraries, as well as libraries for reinforcement learning, clustering, and (optionally) quantum computing simulations.\n",
        "\"\"\"\n",
        "\n",
        "# %%\n",
        "!pip install torch torchvision scikit-learn tensorboard faiss-cpu scikit-optimize\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "## Cell 2: Imports & Logging Configuration\n",
        "Import libraries, set up logging, configure TensorBoard, and import modules for quantum simulation.\n",
        "This final version includes comprehensive console logging and progress indicators.\n",
        "\"\"\"\n",
        "# %%\n",
        "#######################\n",
        "# Imports & Logging   #\n",
        "#######################\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "import json\n",
        "import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split  # Make sure this is imported!\n",
        "\n",
        "# For Bayesian optimization and progress indication\n",
        "from sklearn.base import BaseEstimator\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real\n",
        "from tqdm import tqdm\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "writer = SummaryWriter('runs/Combined_Experiment')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# Cell 3: Data Preprocessing & Augmentation (Phase 2.5 - Dataset Diversification)\n",
        "This cell loads one of several datasets. You can choose from:\n",
        "  - Standard datasets: \"Iris\", \"Wine\", \"Titanic\", \"Digits\", \"Fashion-MNIST\"\n",
        "  - Multimodal datasets: \"MELD\", \"IEMOCAP\", \"LUMA\", \"M3AV\"\n",
        "For multimodal datasets, we assume a CSV file is available with (at least) a 'text' and 'label' column.\n",
        "For standard datasets, we use scikit‑learn or torchvision.\n",
        "\"\"\"\n",
        "# %%\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set your desired dataset name here:\n",
        "# Options for standard datasets: \"Iris\", \"Wine\", \"Titanic\", \"Digits\", \"Fashion-MNIST\"\n",
        "# Options for multimodal datasets: \"MELD\", \"IEMOCAP\", \"LUMA\", \"M3AV\"\n",
        "dataset_name = \"Iris\"  # <-- change this value to test different datasets\n",
        "\n",
        "multimodal_datasets = {\"MELD\", \"IEMOCAP\", \"LUMA\", \"M3AV\"}\n",
        "enable_multimodal = dataset_name in multimodal_datasets\n",
        "\n",
        "if dataset_name == \"MELD\":\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        df = pd.read_csv('meld_data.csv')\n",
        "        vectorizer = TfidfVectorizer(max_features=10)\n",
        "        text_features = vectorizer.fit_transform(df['text']).toarray()\n",
        "        np.random.seed(42)\n",
        "        audio_features = np.random.normal(0, 1, size=(df.shape[0], 3))\n",
        "        video_features = np.random.normal(0, 1, size=(df.shape[0], 5))\n",
        "        X = np.concatenate([text_features, audio_features, video_features], axis=1)\n",
        "        y = df['label'].values\n",
        "        print(\"Loaded MELD dataset.\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not load MELD dataset; falling back to Iris.\")\n",
        "        dataset_name = \"Iris\"\n",
        "        enable_multimodal = False\n",
        "\n",
        "elif dataset_name == \"IEMOCAP\":\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        df = pd.read_csv('iemocap_data.csv')\n",
        "        vectorizer = TfidfVectorizer(max_features=10)\n",
        "        text_features = vectorizer.fit_transform(df['text']).toarray()\n",
        "        np.random.seed(42)\n",
        "        audio_features = np.random.normal(0, 1, size=(df.shape[0], 3))\n",
        "        video_features = np.random.normal(0, 1, size=(df.shape[0], 5))\n",
        "        X = np.concatenate([text_features, audio_features, video_features], axis=1)\n",
        "        y = df['label'].values\n",
        "        print(\"Loaded IEMOCAP dataset.\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not load IEMOCAP dataset; falling back to Iris.\")\n",
        "        dataset_name = \"Iris\"\n",
        "        enable_multimodal = False\n",
        "\n",
        "elif dataset_name == \"LUMA\":\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        df = pd.read_csv('luma_data.csv')\n",
        "        vectorizer = TfidfVectorizer(max_features=10)\n",
        "        text_features = vectorizer.fit_transform(df['text']).toarray()\n",
        "        np.random.seed(42)\n",
        "        image_features = np.random.normal(0, 1, size=(df.shape[0], 4))\n",
        "        audio_features = np.random.normal(0, 1, size=(df.shape[0], 3))\n",
        "        X = np.concatenate([text_features, image_features, audio_features], axis=1)\n",
        "        y = df['label'].values\n",
        "        print(\"Loaded LUMA dataset.\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not load LUMA dataset; falling back to Iris.\")\n",
        "        dataset_name = \"Iris\"\n",
        "        enable_multimodal = False\n",
        "\n",
        "elif dataset_name == \"M3AV\":\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        df = pd.read_csv('m3av_data.csv')\n",
        "        vectorizer = TfidfVectorizer(max_features=10)\n",
        "        text_features = vectorizer.fit_transform(df['text']).toarray()\n",
        "        np.random.seed(42)\n",
        "        speech_features = np.random.normal(0, 1, size=(df.shape[0], 3))\n",
        "        visual_features = np.random.normal(0, 1, size=(df.shape[0], 5))\n",
        "        X = np.concatenate([text_features, speech_features, visual_features], axis=1)\n",
        "        y = df['label'].values\n",
        "        print(\"Loaded M3AV dataset.\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not load M3AV dataset; falling back to Iris.\")\n",
        "        dataset_name = \"Iris\"\n",
        "        enable_multimodal = False\n",
        "\n",
        "elif dataset_name == \"Iris\":\n",
        "    from sklearn import datasets\n",
        "    iris = datasets.load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    np.random.seed(42)\n",
        "    extra_features = np.random.normal(0, 1, size=(X.shape[0], 3))\n",
        "    X = np.concatenate([X, extra_features], axis=1)\n",
        "    print(\"Using Iris dataset with simulated extra features.\")\n",
        "\n",
        "elif dataset_name == \"Wine\":\n",
        "    from sklearn import datasets\n",
        "    wine = datasets.load_wine()\n",
        "    X = wine.data\n",
        "    y = wine.target\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    np.random.seed(42)\n",
        "    extra_features = np.random.normal(0, 1, size=(X.shape[0], 2))\n",
        "    X = np.concatenate([X, extra_features], axis=1)\n",
        "    print(\"Using Wine dataset with simulated extra features.\")\n",
        "\n",
        "elif dataset_name == \"Titanic\":\n",
        "    try:\n",
        "        import seaborn as sns\n",
        "        df = sns.load_dataset(\"titanic\").dropna()\n",
        "        X = df[['pclass', 'age', 'sibsp', 'parch', 'fare']].values\n",
        "        y = df[\"survived\"].values\n",
        "        scaler = StandardScaler()\n",
        "        X = scaler.fit_transform(X)\n",
        "        np.random.seed(42)\n",
        "        extra_features = np.random.normal(0, 1, size=(X.shape[0], 2))\n",
        "        X = np.concatenate([X, extra_features], axis=1)\n",
        "        print(\"Using Titanic dataset with simulated extra features.\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not load Titanic dataset; falling back to Iris.\")\n",
        "        dataset_name = \"Iris\"\n",
        "        enable_multimodal = False\n",
        "\n",
        "elif dataset_name == \"Digits\":\n",
        "    from sklearn import datasets\n",
        "    digits = datasets.load_digits()\n",
        "    X = digits.data\n",
        "    y = digits.target\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    np.random.seed(42)\n",
        "    extra_features = np.random.normal(0, 1, size=(X.shape[0], 2))\n",
        "    X = np.concatenate([X, extra_features], axis=1)\n",
        "    print(\"Using Digits dataset with simulated extra features.\")\n",
        "\n",
        "elif dataset_name == \"Fashion-MNIST\":\n",
        "    from torchvision import datasets, transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.view(-1))\n",
        "    ])\n",
        "    fashion = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    X = np.array([np.array(img) for img, _ in fashion])\n",
        "    y = np.array([label for _, label in fashion])\n",
        "    X = X / 255.0\n",
        "    X = X.reshape(X.shape[0], -1)\n",
        "    np.random.seed(42)\n",
        "    extra_features = np.random.normal(0, 1, size=(X.shape[0], 2))\n",
        "    X = np.concatenate([X, extra_features], axis=1)\n",
        "    print(\"Using Fashion-MNIST dataset with simulated extra features.\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Dataset '{dataset_name}' not recognized. Please choose from Iris, Wine, Titanic, Digits, Fashion-MNIST, MELD, IEMOCAP, LUMA, or M3AV.\")\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y_tensor)\n",
        "X_train = X_train.to(device)\n",
        "X_val = X_val.to(device)\n",
        "y_train = y_train.to(device)\n",
        "y_val = y_val.to(device)\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "print(f\"Dataset: {dataset_name} | Multi‐Modal Enabled: {enable_multimodal}\")\n",
        "print(f\"Training data shape: {X_train.shape}, Validation data shape: {X_val.shape}\")\n",
        "print(f\"Unique classes: {torch.unique(y_tensor)}\")\n",
        "\n",
        "# Compute class weights for bias mitigation\n",
        "class_counts = torch.bincount(y_train)\n",
        "class_weights = 1.0 / (class_counts.float() + 1e-8)\n",
        "class_weights = class_weights / class_weights.sum()\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "def mixup(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    index = torch.randperm(x.size(0)).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    num_classes = int(torch.max(y)) + 1\n",
        "    y_onehot = torch.nn.functional.one_hot(y, num_classes=num_classes).float()\n",
        "    mixed_y = lam * y_onehot + (1 - lam) * y_onehot[index, :]\n",
        "    return mixed_x, mixed_y\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# Cell 4: Utility Functions (Phase 2.5 - Next Iteration)\n",
        "Defines functions for various metrics, uncertainty estimation, fairness, robust loss, zero-shot evaluation,\n",
        "sensor noise simulation, prediction distribution, and a dummy bias correction.\n",
        "\"\"\"\n",
        "# %%\n",
        "def hyperbolic_distance(x, y):\n",
        "    x = torch.tensor(x, dtype=torch.float32, device=device)\n",
        "    y = torch.tensor(y, dtype=torch.float32, device=device)\n",
        "    norm_x = torch.norm(x)\n",
        "    norm_y = torch.norm(y)\n",
        "    return torch.acosh(1 + 2 * torch.sum((x - y)**2) / ((1 - norm_x**2) * (1 - norm_y**2)))\n",
        "\n",
        "def compute_centroid_entropy(centroids):\n",
        "    with torch.no_grad():\n",
        "        D = torch.cdist(centroids, centroids, p=2)\n",
        "        D.fill_diagonal_(float('inf'))\n",
        "        p = 1 / (D + 1e-8)\n",
        "        p = p / p.sum(dim=1, keepdim=True)\n",
        "        p = torch.clamp(p, min=1e-8)\n",
        "        entropy_per_centroid = -torch.sum(p * torch.log(p), dim=1)\n",
        "        return torch.mean(entropy_per_centroid)\n",
        "\n",
        "def enforce_centroid_separation(centroids, min_sep=1.0):\n",
        "    with torch.no_grad():\n",
        "        num_centroids = centroids.size(0)\n",
        "        for i in range(num_centroids):\n",
        "            for j in range(i+1, num_centroids):\n",
        "                diff = centroids[i] - centroids[j]\n",
        "                dist = torch.norm(diff)\n",
        "                if dist < min_sep:\n",
        "                    correction = (min_sep - dist) / 2\n",
        "                    direction = diff / (dist + 1e-8)\n",
        "                    centroids[i].add_(correction * direction)\n",
        "                    centroids[j].add_(-correction * direction)\n",
        "\n",
        "def monte_carlo_dropout(model, x, epoch, num_samples=10):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    for _ in range(num_samples):\n",
        "        with torch.amp.autocast('cuda', enabled=True):\n",
        "            outputs = model(x, epoch)\n",
        "        preds.append(torch.softmax(outputs, dim=1).unsqueeze(0))\n",
        "    preds = torch.cat(preds, dim=0)\n",
        "    mean_preds = preds.mean(dim=0)\n",
        "    variance = preds.var(dim=0).mean().item()\n",
        "    return mean_preds, variance\n",
        "\n",
        "def monte_carlo_dropout_per_sample(model, x, epoch, num_samples=10):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    for _ in range(num_samples):\n",
        "        with torch.amp.autocast('cuda', enabled=True):\n",
        "            outputs = model(x, epoch)\n",
        "        preds.append(torch.softmax(outputs, dim=1).unsqueeze(0))\n",
        "    preds = torch.cat(preds, dim=0)\n",
        "    sample_variances = preds.var(dim=0).mean(dim=1)\n",
        "    return sample_variances\n",
        "\n",
        "def fairness_metric(pred_probs, y_true):\n",
        "    preds = torch.argmax(pred_probs, dim=1)\n",
        "    unique, counts = torch.unique(y_true, return_counts=True)\n",
        "    ideal = torch.ones_like(counts, dtype=torch.float32) / len(unique)\n",
        "    actual = counts.float() / counts.sum()\n",
        "    bias = torch.norm(actual - ideal)\n",
        "    return bias.item()\n",
        "\n",
        "def log_energy_usage():\n",
        "    usage = torch.cuda.memory_allocated(device) / 1e6\n",
        "    return usage\n",
        "\n",
        "def compute_modal_consistency(x):\n",
        "    return 1.0  # For fallback datasets, we return 1.0.\n",
        "\n",
        "def meta_cognition_metric(pred_probs):\n",
        "    entropy = -torch.sum(pred_probs * torch.log(pred_probs + 1e-8), dim=1)\n",
        "    return entropy.mean().item()\n",
        "\n",
        "def compute_saliency_map(model, x, target_class=None):\n",
        "    model.eval()\n",
        "    x.requires_grad_()\n",
        "    outputs = model(x, 0)\n",
        "    if target_class is None:\n",
        "        target_class = outputs.argmax(dim=1)\n",
        "    loss = nn.CrossEntropyLoss()(outputs, target_class)\n",
        "    loss.backward()\n",
        "    saliency = x.grad.abs().mean().item()\n",
        "    x.requires_grad_(False)\n",
        "    return saliency\n",
        "\n",
        "weighted_loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "def robust_loss(x, outputs, targets, model, lambda_reg=0.01):\n",
        "    ce_loss = weighted_loss_fn(outputs, targets)\n",
        "    x_adv = x.clone().detach().requires_grad_(True)\n",
        "    outputs_adv = model(x_adv, 0)\n",
        "    loss_for_grad = weighted_loss_fn(outputs_adv, targets)\n",
        "    loss_for_grad.backward(retain_graph=True)\n",
        "    grad_norm = x_adv.grad.norm(2)\n",
        "    penalty = lambda_reg * grad_norm\n",
        "    return ce_loss + penalty\n",
        "\n",
        "def zero_shot_evaluation(model, num_samples=20):\n",
        "    synthetic_data = generate_zero_shot_data(num_samples, input_size)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(synthetic_data, 0)\n",
        "    predictions = torch.argmax(outputs, dim=1)\n",
        "    unique, counts = torch.unique(predictions, return_counts=True)\n",
        "    return dict(zip(unique.cpu().numpy(), counts.cpu().numpy()))\n",
        "\n",
        "def detect_training_failure(loss_history, patience=5, min_improvement=0.01):\n",
        "    if len(loss_history) < patience:\n",
        "        return False\n",
        "    recent = loss_history[-patience:]\n",
        "    if max(recent) - min(recent) < min_improvement:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def update_experience_replay(buffer, new_batch, max_size=100):\n",
        "    buffer.extend(new_batch)\n",
        "    if len(buffer) > max_size:\n",
        "        buffer = buffer[-max_size:]\n",
        "    return buffer\n",
        "\n",
        "def apply_adaptive_pruning(model, threshold=0.05):\n",
        "    with torch.no_grad():\n",
        "        for module in model.main_head:\n",
        "            if isinstance(module, nn.Linear):\n",
        "                module.weight.data = module.weight.data.masked_fill(module.weight.data.abs() < threshold, 0.0)\n",
        "\n",
        "def simulate_sensor_noise(x, noise_std=0.1):\n",
        "    noise = noise_std * torch.randn_like(x)\n",
        "    return x + noise\n",
        "\n",
        "def apply_bias_correction(pred_probs, bias_threshold=0.1):\n",
        "    return pred_probs\n",
        "\n",
        "def explain_with_shap(model, x):\n",
        "    import numpy as np\n",
        "    return np.random.uniform(0.0, 1.0)\n",
        "\n",
        "def compute_prediction_distribution(model, x, epoch=0):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x, epoch)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "    import numpy as np\n",
        "    unique, counts = np.unique(preds, return_counts=True)\n",
        "    return dict(zip(unique, counts))\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# Cell 5: RL Hyperparameter Agent (Phase 2.5 - Next Iteration)\n",
        "Updates hyperparameters based on various metrics.\n",
        "\"\"\"\n",
        "# %%\n",
        "class RLHyperparameterAgent:\n",
        "    def __init__(self, init_alpha=0.1, init_lr=0.01, energy_threshold=25.0):\n",
        "        self.alpha = init_alpha\n",
        "        self.lr = init_lr\n",
        "        self.max_centroid_sep = 20\n",
        "        self.energy_threshold = energy_threshold\n",
        "    def update(self, centroid_separation, fairness=None, modal_consistency=None, meta_cognition=None, energy_usage=None):\n",
        "        if fairness is not None and fairness > 0.1:\n",
        "            self.alpha = min(0.5, self.alpha * 1.06)\n",
        "            self.lr = min(0.05, self.lr * 1.06)\n",
        "        elif modal_consistency is not None and modal_consistency < 0.5:\n",
        "            self.alpha = min(0.5, self.alpha * 1.04)\n",
        "            self.lr = min(0.05, self.lr * 1.04)\n",
        "        elif meta_cognition is not None and meta_cognition < 0.8:\n",
        "            self.alpha = min(0.5, self.alpha * 1.05)\n",
        "            self.lr = min(0.05, self.lr * 1.05)\n",
        "        elif energy_usage is not None and energy_usage > self.energy_threshold:\n",
        "            self.lr = max(1e-4, self.lr * 0.95)\n",
        "        elif centroid_separation < 1.0:\n",
        "            self.alpha = min(0.5, self.alpha * 1.03)\n",
        "            self.lr = min(0.05, self.lr * 1.01)\n",
        "        else:\n",
        "            self.alpha = max(0.01, self.alpha * 0.99)\n",
        "            self.lr = max(1e-4, self.lr * 0.99)\n",
        "        centroid_separation = min(self.max_centroid_sep, centroid_separation)\n",
        "        return self.alpha, self.lr, centroid_separation\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# Cell 6: Evolvable Model Architectures (NAS Module) (Phase 2.5 - Next Iteration)\n",
        "Builds an evolvable MLP candidate with support for few-shot mode, adaptive pruning, and meta-cognition.\n",
        "Also adds a method to mutate the architecture.\n",
        "\"\"\"\n",
        "# %%\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "def build_evolvable_model(candidate, input_size, num_classes, total_epochs=20):\n",
        "    arch = candidate[\"architecture\"]\n",
        "    few_shot = candidate.get(\"few_shot\", False)\n",
        "    adaptive_pruning = candidate.get(\"adaptive_pruning\", False)\n",
        "    meta_cognition_enabled = candidate.get(\"meta_cognition\", False)\n",
        "    if arch == \"MLP\":\n",
        "        class EvolvableMLP(nn.Module):\n",
        "            def __init__(self, candidate, input_size, num_classes, total_epochs):\n",
        "                super(EvolvableMLP, self).__init__()\n",
        "                self.candidate = candidate  # store candidate for cloning\n",
        "                self.adaptive_pruning = adaptive_pruning\n",
        "                self.meta_cognition_enabled = meta_cognition_enabled\n",
        "                layers = []\n",
        "                in_features = input_size\n",
        "                if few_shot:\n",
        "                    meta_hidden = max(16, candidate[\"hidden_size\"] // 2)\n",
        "                    layers.append(nn.Linear(in_features, meta_hidden))\n",
        "                    layers.append(nn.ReLU())\n",
        "                    in_features = meta_hidden\n",
        "                num_layers = candidate[\"num_layers\"]\n",
        "                hidden_size = candidate[\"hidden_size\"]\n",
        "                dropout_rate = candidate[\"dropout\"]\n",
        "                activations = [nn.ReLU(), nn.SiLU(), nn.LeakyReLU()]\n",
        "                for i in range(num_layers):\n",
        "                    layers.append(nn.Linear(in_features, hidden_size))\n",
        "                    layers.append(random.choice(activations))\n",
        "                    layers.append(nn.Dropout(dropout_rate))\n",
        "                    in_features = hidden_size\n",
        "                self.main_head = nn.Sequential(*layers)\n",
        "                self.classifier = nn.Linear(in_features, num_classes)\n",
        "                if self.meta_cognition_enabled:\n",
        "                    self.reflection_head = nn.Linear(in_features, 1)\n",
        "                self.total_epochs = total_epochs\n",
        "\n",
        "            def forward(self, x, epoch=None):\n",
        "                features = self.main_head(x)\n",
        "                out = self.classifier(features)\n",
        "                if self.meta_cognition_enabled:\n",
        "                    self.reflection_output = self.reflection_head(features)\n",
        "                self.latent_features = features\n",
        "                return out\n",
        "\n",
        "            def mutate_architecture(self):\n",
        "                for module in self.main_head:\n",
        "                    if isinstance(module, nn.Dropout):\n",
        "                        new_rate = module.p + random.uniform(-0.05, 0.05)\n",
        "                        module.p = max(0.0, min(new_rate, 0.5))\n",
        "                        break\n",
        "\n",
        "        return EvolvableMLP(candidate, input_size, num_classes, total_epochs)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown architecture in candidate: {arch}\")\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# Cell 7: Custom Estimator for Hyperparameter Optimization & Automated Retraining (Phase 2.5 - Next Iteration)\n",
        "This estimator integrates multiple self-improvement features.\n",
        "Note: Instead of using copy.deepcopy, we now use a custom cloning function to re-create a fresh model from the stored candidate.\n",
        "\"\"\"\n",
        "# %%\n",
        "from sklearn.base import BaseEstimator\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def adaptive_alpha(epoch, base_alpha=0.01, increase_factor=1.05, max_alpha=0.1):\n",
        "    return min(base_alpha * (increase_factor ** epoch), max_alpha)\n",
        "\n",
        "def neural_network_competition(X, y, population, num_epochs_comp=3, batch_size=16):\n",
        "    best_model = None\n",
        "    best_loss = float('inf')\n",
        "    for model in population:\n",
        "        model.train()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "        total_loss = 0.0\n",
        "        for comp_epoch in range(num_epochs_comp):\n",
        "            permutation = torch.randperm(X.size(0))\n",
        "            for i in range(0, X.size(0), batch_size):\n",
        "                indices = permutation[i:i+batch_size]\n",
        "                batch = X[indices]\n",
        "                batch_labels = y[indices]\n",
        "                optimizer.zero_grad()\n",
        "                with torch.amp.autocast('cuda', enabled=True):\n",
        "                    outputs = model(batch, comp_epoch)\n",
        "                    loss = nn.CrossEntropyLoss(weight=class_weights)(outputs, batch_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "        if total_loss < best_loss:\n",
        "            best_loss = total_loss\n",
        "            best_model = model.state_dict()  # store state dict of best model\n",
        "    return best_model\n",
        "\n",
        "class MANEEstimator(BaseEstimator):\n",
        "    def __init__(self, input_size, hidden_size, latent_size, num_classes,\n",
        "                 entropy_weight=0.1, min_sep=1.0, base_alpha=0.01, total_epochs=35,\n",
        "                 confidence_threshold=0.05, use_curriculum=True, self_directed=False):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.latent_size = latent_size\n",
        "        self.num_classes = num_classes\n",
        "        self.entropy_weight = entropy_weight\n",
        "        self.min_sep = min_sep\n",
        "        self.base_alpha = base_alpha\n",
        "        self.total_epochs = total_epochs\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.use_curriculum = use_curriculum\n",
        "        self.self_directed = self_directed\n",
        "        # Build model with candidate dictionary\n",
        "        self.candidate = {\n",
        "            \"architecture\": \"MLP\",\n",
        "            \"num_layers\": 3,\n",
        "            \"hidden_size\": hidden_size,\n",
        "            \"dropout\": 0.35,\n",
        "            \"few_shot\": False,\n",
        "            \"adaptive_pruning\": False,\n",
        "            \"meta_cognition\": True\n",
        "        }\n",
        "        self.model = build_evolvable_model(self.candidate, input_size, num_classes, total_epochs=self.total_epochs)\n",
        "        self.model.to(device)\n",
        "        self.optimizer = None\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        self.best_model = None\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.no_improve_epochs = 0\n",
        "        self.experience_replay_buffer = []\n",
        "        self.loss_history = []\n",
        "\n",
        "    def fit(self, X, y, learning_rate=0.005, num_epochs=35, batch_size=64, weight_decay=5e-3,\n",
        "            patience=5, auto_grok_min_epochs=10, auto_grok_threshold=0.04):\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "        self.no_improve_epochs = 0\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=2, T_mult=1, eta_min=5e-6)\n",
        "\n",
        "        # Cloning function to safely duplicate model without deepcopy()\n",
        "        def clone_fn(model):\n",
        "            new_model = build_evolvable_model(model.candidate, self.input_size, self.num_classes, self.total_epochs)\n",
        "            new_model.load_state_dict(model.state_dict())\n",
        "            new_model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "            return new_model\n",
        "\n",
        "        # Instead of deepcopy, safely clone model using clone_fn()\n",
        "        population = [clone_fn(self.model) for _ in range(3)]\n",
        "\n",
        "        # Reset parameters if necessary\n",
        "        for clone in population:\n",
        "            try:\n",
        "                clone.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Run neural network competition\n",
        "        best_population_state = neural_network_competition(X, y, population, num_epochs_comp=3, batch_size=batch_size)\n",
        "        if best_population_state is not None:\n",
        "            self.model.load_state_dict(best_population_state)\n",
        "\n",
        "        self.loss_history = []\n",
        "        exp_buffer = []  # Local experience replay buffer\n",
        "        for epoch in tqdm(range(num_epochs), desc=\"Retraining Epochs\"):\n",
        "            current_alpha = adaptive_alpha(epoch, base_alpha=0.01)\n",
        "            if hasattr(self.model, 'base_alpha'):\n",
        "                self.model.base_alpha = current_alpha\n",
        "\n",
        "            if self.use_curriculum:\n",
        "                curriculum_X, curriculum_y = generate_curriculum(X, y, self.model, num_samples=int(0.1 * X.size(0)))\n",
        "                X_epoch = torch.cat([X, curriculum_X])\n",
        "                y_epoch = torch.cat([y, curriculum_y])\n",
        "            else:\n",
        "                X_epoch, y_epoch = X, y\n",
        "\n",
        "            if epoch % 3 == 0:\n",
        "                self.model.mutate_architecture()\n",
        "                adv_examples = generate_adversarial_examples(X_epoch, y_epoch, self.model, epsilon=0.05)\n",
        "                X_epoch = torch.cat([X_epoch, adv_examples])\n",
        "                y_epoch = torch.cat([y_epoch, y_epoch])\n",
        "\n",
        "            self.model.train()\n",
        "            permutation = torch.randperm(X_epoch.size(0))\n",
        "            epoch_loss = 0.0\n",
        "            for i in range(0, X_epoch.size(0), batch_size):\n",
        "                indices = permutation[i:i+batch_size]\n",
        "                batch = X_epoch[indices]\n",
        "                batch_labels = y_epoch[indices]\n",
        "                self.optimizer.zero_grad()\n",
        "                with torch.amp.autocast('cuda', enabled=True):\n",
        "                    outputs = self.model(batch, epoch)\n",
        "                    loss = robust_loss(batch, outputs, batch_labels, self.model, lambda_reg=0.01)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                self.optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "                exp_buffer = update_experience_replay(exp_buffer, [(batch, batch_labels)])\n",
        "\n",
        "            scheduler.step()\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(X, epoch)\n",
        "                val_loss = self.criterion(outputs, y).mean()\n",
        "            self.loss_history.append(val_loss.item())\n",
        "\n",
        "            print(f\"Epoch {epoch+1}: Train Loss={epoch_loss:.4f}, Val Loss={val_loss.item():.4f}\")\n",
        "\n",
        "            if self.best_val_loss - val_loss.item() > 0.01:\n",
        "                self.no_improve_epochs = 0\n",
        "                self.best_val_loss = val_loss.item()\n",
        "                self.best_model = self.model.state_dict()\n",
        "            else:\n",
        "                self.no_improve_epochs += 1\n",
        "                if self.no_improve_epochs >= patience:\n",
        "                    print(\"Early stopping triggered.\")\n",
        "                    break\n",
        "\n",
        "        if self.best_model is not None:\n",
        "            self.model.load_state_dict(self.best_model)\n",
        "\n",
        "        stats_report = {\"retraining_runs\": []}\n",
        "        stats_report[\"retraining_runs\"].append({\n",
        "            \"run\": epoch+1,\n",
        "            \"best_val_loss\": self.best_val_loss,\n",
        "            \"total_epochs\": num_epochs\n",
        "        })\n",
        "\n",
        "        print(f\"Retraining run complete.\")\n",
        "        return self.model, stats_report\n",
        "\n",
        "# Instantiate and run training\n",
        "mane_model, retraining_stats = automated_retraining(X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Simple wrapper for AutoML evaluation\n",
        "from sklearn.base import BaseEstimator\n",
        "class DummyEstimator(BaseEstimator):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "    def score(self, X, y):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X, 0)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            return (preds == y).float().mean().item()\n",
        "\n",
        "estimator_wrapper = DummyEstimator(mane_model)\n",
        "automl_accuracy = estimator_wrapper.score(X_val, y_val)\n",
        "print(f\"AutoML Selected Model Accuracy: {automl_accuracy*100:.2f}%\")\n",
        "\n",
        "mane_writer.add_scalar(\"Final_Val_Accuracy\", automl_accuracy)\n",
        "mane_writer.close()\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "X_baseline = X_train.cpu().numpy()\n",
        "y_baseline = y_train.cpu().numpy()\n",
        "X_val_baseline = X_val.cpu().numpy()\n",
        "y_val_baseline = y_val.cpu().numpy()\n",
        "baseline_model = LogisticRegression(max_iter=400)\n",
        "baseline_model.fit(X_baseline, y_baseline)\n",
        "baseline_preds = baseline_model.predict(X_val_baseline)\n",
        "baseline_probs = baseline_model.predict_proba(X_val_baseline)\n",
        "baseline_acc = accuracy_score(y_val_baseline, baseline_preds)\n",
        "baseline_logloss = log_loss(y_val_baseline, baseline_probs)\n",
        "print(f\"Baseline Accuracy: {baseline_acc*100:.2f}%\")\n",
        "print(f\"Baseline Log Loss: {baseline_logloss:.4f}\")\n",
        "baseline_writer.add_scalar(\"Final_Val_Accuracy\", baseline_acc)\n",
        "baseline_writer.add_scalar(\"Final_Log_Loss\", baseline_logloss)\n",
        "baseline_writer.close()\n",
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# Cell 10: Evaluate and Visualize the Learned Latent Space (Phase 2.5 - Next Iteration)\n",
        "Visualizes the latent space using PCA and annotates energy usage, modal consistency, meta‑cognition,\n",
        "and saliency. It also simulates edge deployment by reporting the model’s parameter count and inference time,\n",
        "and plots a histogram of the prediction distribution (for bias analysis).\n",
        "\"\"\"\n",
        "# %%\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import time\n",
        "\n",
        "def simulate_edge_deployment(model):\n",
        "    model_cpu = model.to(\"cpu\")\n",
        "    param_count = sum(p.numel() for p in model_cpu.parameters())\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        dummy_input = torch.randn(1, input_size)\n",
        "        _ = model_cpu(dummy_input, 0)\n",
        "    inference_time = time.time() - start\n",
        "    return param_count, inference_time\n",
        "\n",
        "def reset_vram():\n",
        "    print(\"🔄 Resetting VRAM...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    print(\"✅ VRAM has been cleared.\")\n",
        "\n",
        "if hasattr(mane_model, 'latent_features') and mane_model.latent_features is not None:\n",
        "    mane_model.eval()\n",
        "    with torch.no_grad():\n",
        "        _ = mane_model(X_train, 0)\n",
        "        latent_reps = mane_model.latent_features.cpu().numpy()\n",
        "    pca = PCA(n_components=2)\n",
        "    latent_2d = pca.fit_transform(latent_reps)\n",
        "    y_train_np = y_train.cpu().numpy()\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.scatter(latent_2d[:,0], latent_2d[:,1], c=y_train_np, cmap='viridis', alpha=0.7)\n",
        "    if hasattr(mane_model, 'centroids'):\n",
        "        centroids_np = mane_model.centroids.detach().cpu().numpy()\n",
        "        centroids_2d = pca.transform(centroids_np)\n",
        "        plt.scatter(centroids_2d[:,0], centroids_2d[:,1], marker='x', s=200, c='red', label='Centroids')\n",
        "    energy_usage = log_energy_usage()\n",
        "    modal_consistency = compute_modal_consistency(X_train)\n",
        "    meta_score = meta_cognition_metric(torch.softmax(mane_model(X_train, 0), dim=1))\n",
        "    sample_saliency = compute_saliency_map(mane_model, X_val[:10], target_class=y_val[:10])\n",
        "    param_count, inference_time = simulate_edge_deployment(mane_model)\n",
        "    pred_dist = compute_prediction_distribution(mane_model, X_train, epoch=0)\n",
        "    plt.figtext(0.15, 0.85, f\"Energy: {energy_usage:.2f} MB\\nModal Consistency: {modal_consistency:.2f}\\nMeta-Cognition: {meta_score:.2f}\\nSaliency: {sample_saliency:.2f}\\nParams: {param_count}\\nInference: {inference_time*1000:.2f} ms\\nPred Dist: {pred_dist}\", fontsize=10)\n",
        "    plt.title(f\"Latent Space Visualization for {dataset_name}\")\n",
        "    plt.xlabel(\"PCA Component 1\")\n",
        "    plt.ylabel(\"PCA Component 2\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"The selected architecture does not provide latent features for visualization.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJYCAvfPULHv",
        "outputId": "6fb3c265-fd3e-4edb-9ac0-2378feb9e4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Iris dataset with simulated extra features.\n",
            "Dataset: Iris | Multi‐Modal Enabled: False\n",
            "Training data shape: torch.Size([120, 7]), Validation data shape: torch.Size([30, 7])\n",
            "Unique classes: tensor([0, 1, 2])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retraining Epochs:  20%|██        | 4/20 [00:00<00:00, 16.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss=3.3771, Val Loss=0.2207\n",
            "Epoch 2: Train Loss=1.4303, Val Loss=0.1859\n",
            "Epoch 3: Train Loss=1.5000, Val Loss=0.1416\n",
            "Epoch 4: Train Loss=2.5788, Val Loss=0.1047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRetraining Epochs:  35%|███▌      | 7/20 [00:00<00:00, 19.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss=1.0207, Val Loss=0.0873\n",
            "Epoch 6: Train Loss=0.9102, Val Loss=0.0753\n",
            "Epoch 7: Train Loss=1.8647, Val Loss=0.1583\n",
            "Epoch 8: Train Loss=1.3028, Val Loss=0.0846\n",
            "Epoch 9: Train Loss=0.9511, Val Loss=0.1064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retraining Epochs:  65%|██████▌   | 13/20 [00:00<00:00, 17.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Loss=2.9530, Val Loss=0.0557\n",
            "Epoch 11: Train Loss=0.9651, Val Loss=0.1287\n",
            "Epoch 12: Train Loss=1.7086, Val Loss=0.0769\n",
            "Epoch 13: Train Loss=1.9709, Val Loss=0.0519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rRetraining Epochs:  75%|███████▌  | 15/20 [00:00<00:00, 16.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Train Loss=0.7127, Val Loss=0.0446\n",
            "Epoch 15: Train Loss=0.9196, Val Loss=0.0337\n",
            "Epoch 16: Train Loss=1.3489, Val Loss=0.0278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retraining Epochs: 100%|██████████| 20/20 [00:01<00:00, 16.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: Train Loss=0.5920, Val Loss=0.0265\n",
            "Epoch 18: Train Loss=0.3766, Val Loss=0.0216\n",
            "Epoch 19: Train Loss=1.6262, Val Loss=0.0138\n",
            "Epoch 20: Train Loss=0.4714, Val Loss=0.0132\n",
            "Retraining run complete.\n",
            "Retraining Run 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MANEEstimator' object has no attribute 'state_dict'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-80cd94dbe26a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;31m# Instantiate and run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m \u001b[0mmane_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretraining_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautomated_retraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;31m# Simple wrapper for AutoML evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-31d5bc050074>\u001b[0m in \u001b[0;36mautomated_retraining\u001b[0;34m(X, y, val_X, val_y, retrain_threshold, max_retrain_runs, init_batch_size)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;31m# Use clone_model() instead of deepcopy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mpopulation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclone_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;31m# Reset model parameters if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-31d5bc050074>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;31m# Use clone_model() instead of deepcopy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mpopulation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclone_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;31m# Reset model parameters if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-31d5bc050074>\u001b[0m in \u001b[0;36mclone_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;34m\"\"\"Clones the model safely without using deepcopy\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_evolvable_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Copy weights safely\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move to GPU if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MANEEstimator' object has no attribute 'state_dict'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a-EGqUdCIThe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs/iris/"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EBTrYGe6IR9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs/Wine/"
      ],
      "metadata": {
        "id": "eqibNCoWITB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs/breast_cancer/"
      ],
      "metadata": {
        "id": "Gq-XGPF1ITKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs/mnist/"
      ],
      "metadata": {
        "id": "4iKlAyTpITRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs/fashion_mnist/"
      ],
      "metadata": {
        "id": "rGDQxpn7ITY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# %% [markdown]\n",
        "\"\"\"\n",
        "# Cell 11: VRAM Reset Utility\n",
        "This cell defines a function to reset VRAM by clearing CUDA cache and running garbage collection.\n",
        "Call reset_vram() after each candidate evaluation and retraining run.\n",
        "\"\"\"\n",
        "\n",
        "# %%\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def reset_vram():\n",
        "    \"\"\"Clears VRAM by emptying the CUDA cache and running garbage collection.\"\"\"\n",
        "    print(\"🔄 Resetting VRAM...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"✅ VRAM has been cleared.\")\n",
        "\n",
        "# Example call after a training iteration (already called within candidate loop and retraining).\n",
        "reset_vram()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPIiAcDZrkMq",
        "outputId": "97eee841-0681-4f16-e0e5-ee1ada5d07e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Resetting VRAM...\n",
            "✅ VRAM has been cleared.\n"
          ]
        }
      ]
    }
  ]
}